{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data we will use\n",
    "data1 = 'At eight o\\'clock on Thursday morning, we will go to school. There is' + \\\n",
    "           'No one there but myself. '\n",
    "data2 = \"CSCE 771: Computer Processing of Natural Language \" + \\\n",
    "    \" Lecture 3: Words, Morphology, Lexicons\" + \\\n",
    "    \" Prof. Biplav Srivastava, AI Institute    31st Aug 2020 \"\n",
    "data3 = \"Hola Class! Dost kya kar rahe ho? Let us pay attention.\" ## Mixed language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: tokens - \n",
      "\t['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', ',', 'we', 'will', 'go', 'to', 'school', '.', 'There', 'isNo', 'one', 'there', 'but', 'myself', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(data1)\n",
    "print (\"INFO: tokens - \\n\\t\" + str(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: tokens - \n",
      "\t['CSCE', '771', ':', 'Computer', 'Processing', 'of', 'Natural', 'Language', 'Lecture', '3', ':', 'Words', ',', 'Morphology', ',', 'Lexicons', 'Prof.', 'Biplav', 'Srivastava', ',', 'AI', 'Institute', '31st', 'Aug', '2020']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(data2)\n",
    "print (\"INFO: tokens - \\n\\t\" + str(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: tokens - \n",
      "\t['Hola', 'Class', '!', 'Dost', 'kya', 'kar', 'rahe', 'ho', '?', 'Let', 'us', 'pay', 'attention', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(data3)\n",
    "print (\"INFO: tokens - \\n\\t\" + str(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to get word types and print in sorted form\n",
    "def get_word_types(data):\n",
    "    \n",
    "    tokens = nltk.word_tokenize(data)\n",
    "    tokens = set([t for t in tokens if t.strip()])\n",
    "\n",
    "    tokens = sorted(tokens)\n",
    "    print (\"INFO: sorted unique words - \\n\\t\" + str(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: sorted unique words - \n",
      "\t[',', '.', 'At', 'There', 'Thursday', 'but', 'eight', 'go', 'isNo', 'morning', 'myself', \"o'clock\", 'on', 'one', 'school', 'there', 'to', 'we', 'will']\n"
     ]
    }
   ],
   "source": [
    "get_word_types(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: sorted unique words - \n",
      "\t[',', '2020', '3', '31st', '771', ':', 'AI', 'Aug', 'Biplav', 'CSCE', 'Computer', 'Institute', 'Language', 'Lecture', 'Lexicons', 'Morphology', 'Natural', 'Processing', 'Prof.', 'Srivastava', 'Words', 'of']\n"
     ]
    }
   ],
   "source": [
    "get_word_types(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hola', 'NNP'),\n",
       " ('Class', 'NN'),\n",
       " ('!', '.'),\n",
       " ('Dost', 'NNP'),\n",
       " ('kya', 'VBD'),\n",
       " ('kar', 'JJ'),\n",
       " ('rahe', 'NN'),\n",
       " ('ho', 'NN'),\n",
       " ('?', '.'),\n",
       " ('Let', 'VB'),\n",
       " ('us', 'PRP'),\n",
       " ('pay', 'VB'),\n",
       " ('attention', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('They', 'PRP'),\n",
       " ('refuse', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('permit', 'VB'),\n",
       " ('us', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('obtain', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('refuse', 'NN'),\n",
       " ('permit', 'NN')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Has homonym - permit\n",
    "tokens = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for - churches : church\n",
      "Lemma for - rocks : rock\n",
      "Lemma for - children : child\n",
      "Lemma for - focii : focii\n"
     ]
    }
   ],
   "source": [
    "data_list = ['churches', 'rocks', 'children', 'focii']\n",
    "for data in data_list:\n",
    "    print(\"Lemma for - \" + data + \" : \" + lemmatizer.lemmatize(data)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for example compressed and compression are both accepted a equivalent to compress .\n"
     ]
    }
   ],
   "source": [
    "# Stemmed example for a larger data\n",
    "data = \" for example compressed and compression are both accepted as equivalent to compress.\"\n",
    "tokens = nltk.word_tokenize(data) \n",
    "lemmed_data = \"\"\n",
    "for token in tokens:\n",
    "    lemmed_data = lemmed_data + \" \" + lemmatizer.lemmatize(token)\n",
    "print(lemmed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program  :  program\n",
      "programs  :  program\n",
      "programer  :  program\n",
      "programing  :  program\n",
      "programers  :  program\n"
     ]
    }
   ],
   "source": [
    "# From https://www.geeksforgeeks.org/python-stemming-words-with-nltk\n",
    "from nltk.stem import PorterStemmer \n",
    "ps = PorterStemmer() \n",
    "  \n",
    "# choose some words to be stemmed \n",
    "words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"] \n",
    "  \n",
    "for w in words: \n",
    "    print(w, \" : \", ps.stem(w)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for exampl compress and compress are both accept as equival to compress .\n"
     ]
    }
   ],
   "source": [
    "# Stemmed example\n",
    "data = \" for example compressed and compression are both accepted as equivalent to compress.\"\n",
    "tokens = nltk.word_tokenize(data) \n",
    "stemmed_data = \"\"\n",
    "for token in tokens:\n",
    "    stemmed_data = stemmed_data + \" \" + ps.stem(token)\n",
    "print(stemmed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
